{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN_Exercise.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"z_Zkh2flnBH9","colab_type":"text"},"source":["The following cells are required to enable the display of Gym environment in Colab and install retro.\n","If you run this notebook in local environment, please ignore these cells.\n","\n"]},{"cell_type":"code","metadata":{"id":"_80C2UW1mx4d","colab_type":"code","colab":{}},"source":["!apt-get install -y xvfb python-opengl > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9eeflHXnFlB","colab_type":"code","colab":{}},"source":["!pip3 install gym pyvirtualdisplay > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9juPMUkRnNh3","colab_type":"code","colab":{}},"source":["!pip3 install gym-retro"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3P8v9vksnHFo","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","from IPython import display as ipythondisplay\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(400, 300))\n","display.start()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3SJrodUanXhG","colab_type":"text"},"source":["Make sure no error message is shown in Colab after running the cells above.\n","\n","-------------------------------------------------------------------------------\n","\n","Now import the necessary packages"]},{"cell_type":"code","metadata":{"id":"r5B_qn2onJ7y","colab_type":"code","colab":{}},"source":["import random\n","import gym\n","import numpy as np\n","import cv2\n","import datetime\n","from collections import deque\n","from keras.models import Sequential, clone_model\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","from keras.optimizers import Adam\n","from keras import losses\n","from keras.callbacks import TensorBoard, Callback\n","import tensorflow as tf\n","import os\n","import retro"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3MgJ158nt2c","colab_type":"code","colab":{}},"source":["#define some constants\n","EPISODES = 2000\n","FRAME_NUM = 4\n","RESIZE = 80\n","SKIP_FRAME = 4\n","SHOOT_FRAME = 5\n","ACTION_SPACE = 2 # move left / right\n","MAX_STEP = 10000\n","BATCH_SIZE = 32\n","REWARD_BASE = 1 #5000\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DbATejlGn-34","colab_type":"code","colab":{}},"source":["# for logging training loss history in keras\n","class LossHistory(Callback):\n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","\n","    def on_batch_end(self, batch, logs={}):\n","        self.losses.append(logs.get('loss'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MbfRCg2Dnx8f","colab_type":"text"},"source":["This is the class of the DQN Agent. It contains the definition of the operations of the agents. You are going to implement some of the areas."]},{"cell_type":"code","metadata":{"id":"AJhsB59fn8iI","colab_type":"code","colab":{}},"source":["# DQN Agent\n","class DQNAgent:\n","    def __init__(self, state_size, action_size, batch_size = 32):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=100000)\n","        self.gamma = 0.9    # discount rate\n","        self.epsilon_max = 1.0  # initial exploration rate\n","        self.epsilon_min = 0.1\n","        self.epsilon_decay = 0.001\n","        self.learning_rate = 0.001\n","\n","        self.batch_size = batch_size\n","        self.log_path = './logs'\n","        self.model = self._build_model()\n","        self.target_model = clone_model(self.model)\n","        self.history = LossHistory()\n","        self.observe = 0\n","        self.epsilon = 1.0\n","        self.tensorboard = TensorBoard(log_dir=self.log_path)\n","        self.tensorboard.set_model(self.model)\n","\n","        self.tau_step = 100\n","        self.tau = 0.125\n","        self.loss = 0.0\n","        \n","\n","    def _build_model(self):\n","        # Neural Net for Deep-Q learning Model\n","        model = Sequential()\n","        model.add(Conv2D(32, (8, 8), strides=4, activation = 'elu', input_shape = self.state_size, padding='valid', kernel_initializer='glorot_normal'))\n","        \n","        ### Your code here #####\n","        \n","\n","        ########################\n","        model.compile(loss='mean_squared_error', optimizer=Adam(lr=self.learning_rate))\n","\n","        model.summary()\n","\n","        return model\n","\n","    def remember(self, state, action_id, reward, next_state, done):\n","        ### Your code here ###\n","        # append the parameters as a tuple into the deque self.memory\n","        \n","\n","    def move(self, state, decay_step):\n","        if len(self.memory) < self.batch_size:\n","            return random.randint(0, self.action_size-1) \n","\n","        self.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay * decay_step)\n","    \n","        if (self.epsilon > np.random.rand()):\n","            # Make a random action (exploration)\n","            action_id = random.randint(0,self.action_size-1)\n","        else:\n","            # Get action from Q-network (exploitation)\n","            action_id = np.argmax(self.model.predict(np.reshape(state, (1, *self.state_size))))\n","        return action_id\n","\n","    def trained_move(self, state):\n","        return np.argmax(self.model.predict(np.reshape(state, (1, *self.state_size))))\n","\n","    def train(self, e): \n","        if len(self.memory) < self.batch_size:\n","            return\n","\n","        batch = random.sample(self.memory, self.batch_size)\n","\n","        # split each elements from minibatch\n","        states_mb = np.array([each[0] for each in batch], ndmin=3)\n","        actions_mb = np.array([each[1] for each in batch])\n","        rewards_mb = np.array([each[2] for each in batch]) \n","        next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n","        dones_mb = np.array([each[4] for each in batch])\n","        target_Qs_batch = []\n","\n","        # Get Q values for next_state using the target network \n","        ##### Your code here ####\n","        Qs_next_state = None\n","        #########################\n","        targets_mb = np.squeeze(self.target_model.predict(np.reshape(states_mb, (self.batch_size, *self.state_size))))\n","\n","        # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * maxQ(s', a')\n","        for i in range(self.batch_size):\n","            done = dones_mb[i]\n","            if done:\n","                targets_mb[i,actions_mb[i]] = rewards_mb[i]\n","            else:\n","                targets_mb[i,actions_mb[i]] = rewards_mb[i] + self.gamma * np.max(Qs_next_state[i])\n","\n","        self.loss = self.model.fit(states_mb, targets_mb, epochs = 1, verbose = False)\n","\n","\n","    def update_target_model(self, global_step):\n","        if global_step % self.tau_step == 0 and global_step > self.tau_step:\n","            self.target_model.set_weights(self.model.get_weights())\n","            print(\"Update Target Network in step {}\".format(global_step))\n","\n","    def load_model(self, name):\n","        if os.path.isfile(name):\n","            self.model.load_weights(name)\n","            print(\"Successfully loaded model weights\")\n","        else:\n","            print(\"Can't load the model weights!\")\n","\n","    def save_model(self, name):\n","        self.model.save_weights(name)\n","        print(\"Successfully saved model weights\")\n","\n","\n","    def named_logs(self, model, logs):\n","        result = {} \n","        for l in zip(model.metrics_names, logs):\n","            result[l[0]] = l[1]\n","        return result\n","\n","    def write_log(self, callback, names, logs, batch_no):\n","        for name, value in zip(names, logs):\n","            summary = tf.Summary()\n","            summary_value = summary.value.add()\n","            summary_value.simple_value = value\n","            summary_value.tag = name\n","            callback.writer.add_summary(summary, batch_no)\n","            callback.writer.flush()\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUjxN0rvoiW8","colab_type":"text"},"source":["Now we will go on to define some methods which are useful for processing the game.\n","\n","You can change the delta_score for changing the reward at each step."]},{"cell_type":"code","metadata":{"id":"tYt_AnkCo98X","colab_type":"code","colab":{}},"source":["def check_status(respawn, respawn_prev, lives):\n","    # not dead\n","    killed = False\n","    gameover = False\n","    run = True\n","    delta_score = 0\n","\n","    # keep alive and get some reward\n","    if respawn > 4 and respawn_prev > 4:\n","        #respawn_prev = value\n","        killed = False\n","        run = True\n","        delta_score = 0 # -0.1 #0.01\n","    \n","    # dead, waiting to respawn\n","    if respawn < 4 and respawn_prev < 4:\n","        #respawn_prev = value\n","        killed = False\n","        run = False\n","        delta_score = 0\n","\n","    # respawn\n","    if respawn > 4 and respawn_prev < 4:\n","        killed = False\n","        run = True\n","        delta_score = 0\n","\n","    # just hit and killed    \n","    if respawn < 4 and respawn_prev > 4:\n","        killed = True\n","        run = True\n","        delta_score = 0 #-1000\n","\n","    if killed:\n","        if lives <= 0:\n","            gameover = True\n","            # extra panalty on losing all lives\n","            #delta_score = -10000\n","    \n","    return run, killed, gameover, delta_score\n","\n","def get_action(move, frame_cnt):\n","    action = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","    # keep shooting, no need stop\n","    if frame_cnt % SHOOT_FRAME == 0:\n","        action[0] = 1\n","   \n","    # find action from agent's DQN based on the current state\n","    if move == 0: # move left\n","        action[6] = 1\n","        action[7] = 0\n","    else: # move right\n","        action[6] = 0\n","        action[7] = 1\n","    return action\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfc4akZUonGg","colab_type":"code","colab":{}},"source":["def preprocess_frame(obs):\n","    return cv2.resize(cv2.cvtColor(obs[40:,:,:], cv2.COLOR_BGR2GRAY), (RESIZE, RESIZE), interpolation=cv2.INTER_CUBIC) / 255.0\n","\n","def frames_to_state(frame_queue):\n","    state = np.zeros((RESIZE, RESIZE, FRAME_NUM))\n","    # do frame skipping and find maximum of ith and i-1th frame\n","    for i in range (0, FRAME_NUM):\n","        state[:,:,i] = np.maximum(frame_queue[(i+1)*SKIP_FRAME-1], frame_queue[(i+1)*SKIP_FRAME-2])\n","    return state\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLtTCkjYpbEh","colab_type":"code","colab":{}},"source":["def train():\n","    # create a game environment and initialize it\n","    env = retro.RetroEnv(game='Airstriker-Genesis', use_restricted_actions=retro.Actions.FILTERED  )\n","    #env.reset()\n","\n","    # obtain state parameters\n","    screen_shape = env.observation_space.shape\n","    state_size = (RESIZE, RESIZE, FRAME_NUM)\n","    action_size = ACTION_SPACE\n","\n","    # create game frame container\n","    frame_queue = deque(maxlen=4 * SKIP_FRAME) # 16 frames at most for sampling 1 every 4 frames\n","    for i in range (0, 16):\n","        frame_queue.append(np.zeros((RESIZE, RESIZE)))\n","\n","    # initialize learning agent\n","    agent = DQNAgent(state_size, action_size, batch_size = BATCH_SIZE)\n","    agent.load_model(\"./airstriker_dqn.h5\")\n","\n","    decay_step = -BATCH_SIZE\n","\n","\n","    # begin training episodes\n","    global_step = 0\n","    for e in range(EPISODES):\n","        # initialize the environment for every episode\n","        frame_0 = env.reset()\n","        frame_queue.append(preprocess_frame(frame_0))\n","\n","        # initialize the state (with initial frame)\n","        state = frames_to_state(frame_queue)\n","\n","        # frame counter for counting frame\n","        frame_cnt = 1\n","\n","        # cummulated score of shooting down enemy\n","        score = 0\n","        # no. of hits\n","        hits = 0\n","\n","        gameover = False\n","        killed = False\n","        respawn = 9\n","        respawn_prev = respawn\n","        #if e % 4 == 0:\n","        decay_step = -BATCH_SIZE\n","        \n","\n","        while frame_cnt < MAX_STEP:\n","            screen = env.render(mode='rgb_array')\n","            ##########################\n","            plt.imshow(screen)\n","            ipythondisplay.clear_output(wait=True)\n","            ipythondisplay.display(plt.gcf())\n","            ########################\n","\n","            # get action on the current frame using the last known state\n","            action_id = agent.move(state, global_step )\n","            action = get_action(action_id, frame_cnt)\n","\n","            # get next observation based on the action above\n","            # Note: the reward obtained from env.step() in the game is not the actual reward\n","            # done is not used\n","            next_obs, reward, _, info = env.step(action)\n","            \n","            # append new observation into frame_queue\n","            frame_queue.append(preprocess_frame(next_obs))\n","\n","            # hit an enemy\n","            if int(reward) > 0:\n","                hits += 1\n","                score += 0.05 #1000\n","                #score *= 1.5\n","\n","            # get game state for every SKIP_FRAME-th (e.g. 4) frame\n","            if frame_cnt % SKIP_FRAME == 0:\n","                # check reward for each action taken\n","                # accumulated score (raward) to be stored\n","                respawn = int(info['gameover'])\n","                # check game status and other panalties\n","                run_next_state, killed, gameover, delta_score = check_status(respawn, respawn_prev, int(info['lives']))\n","                #score += delta_score\n","\n","                respawn_prev = respawn\n","\n","                if run_next_state:\n","                    next_state = frames_to_state(frame_queue)    \n","                  \n","                    # save into agent's memory\n","                    agent.remember(state, action_id, score, next_state, killed)\n","                    print(\"Logging: episode: {}, frame_cnt:{}, score:{}, hits:{}, killed:{}, epsilon:{}, mem_len:{}, global_step:{}\".format(e, frame_cnt, score, hits, killed, agent.epsilon, len(agent.memory), global_step))\n","                    state = next_state\n","\n","                    decay_step += 1\n","                else:\n","                    frame_queue.append(np.zeros((RESIZE, RESIZE)))\n","                    next_state = frames_to_state(frame_queue)   \n","                    # save into agent's memory\n","                    agent.remember(state, action_id, score, next_state, killed)\n","                    # reset the hit counter when loosing one lives\n","                    hits = 0\n","                    score = 0\n","\n","                # get experience replay and train agent's and target's model\n","                agent.train(e)\n","                agent.update_target_model(global_step)\n","\n","                global_step += 1\n","\n","                # lost all lives. Restart game immediately\n","                if gameover:\n","                    \n","                    print(\"{} episode: {}/{}, reward: {}, e: {:.2}\" .format(datetime.datetime.now, e, EPISODES, score, agent.epsilon))\n","                    break\n","\n","            frame_cnt += 1\n","\n","        #if e % 10 == 0 and e > 10:\n","        agent.save_model(\"./airstriker_dqn.h5\")\n","\n","    env.close()\n","    ipythondisplay.clear_output(wait=True)\n","    display.stop()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uwZHlNyZpfRb","colab_type":"code","colab":{}},"source":["def play():\n","    # create a game environment and initialize it\n","    env = retro.RetroEnv(game='Airstriker-Genesis', use_restricted_actions=retro.Actions.FILTERED  )\n","    screen = env.render(mode='rgb_array')\n","    ##########################\n","    plt.imshow(screen)\n","    ipythondisplay.clear_output(wait=True)\n","    ipythondisplay.display(plt.gcf())\n","    ########################\n","\n","    # obtain state parameters\n","    screen_shape = env.observation_space.shape\n","    state_size = (RESIZE, RESIZE, FRAME_NUM)\n","    action_size = ACTION_SPACE\n","\n","    # create game frame container\n","    frame_queue = deque(maxlen=4 * SKIP_FRAME) # 16 frames at most for sampling 1 every 4 frames\n","    for i in range (0, 16):\n","        frame_queue.append(np.zeros((RESIZE, RESIZE)))\n","\n","    # initialize learning agent\n","    agent = DQNAgent(state_size, action_size)\n","    agent.load_model(\"./airstriker_dqn.h5\")\n","\n","    # begin training episodes\n","    for e in range(EPISODES):\n","        # initialize the environment for every episode\n","        frame_0 = env.reset()\n","        frame_queue.append(preprocess_frame(frame_0))\n","\n","        # initialize the state (with initial frame)\n","        state = frames_to_state(frame_queue)\n","\n","        # frame counter for counting frame\n","        frame_cnt = 1\n","\n","        gameover = False\n","        killed = False\n","        respawn = 9\n","        respawn_prev = respawn\n","\n","        while True:\n","            env.render()\n","\n","            # get action on the current frame using the last known state\n","            action_id = agent.trained_move(state)\n","            action = get_action(action_id, frame_cnt)\n","\n","            # get next observation based on the action above\n","            next_obs, reward, _, info = env.step(action)\n","            print(frame_cnt, \": \", action_id, reward, \"  \",  info)\n","\n","            respawn = int(info['gameover'])\n","            # check game status and other panalties\n","            run_next_state, killed, gameover, delta_score = check_status(respawn, respawn_prev, int(info['lives']))\n","            # append new observation into frame_queue\n","\n","            frame_queue.append(preprocess_frame(next_obs))\n","            state = frames_to_state(frame_queue)   \n","\n","            if gameover:\n","                print(\"{} episode: {}/{}\" .format(datetime.datetime.now, e, EPISODES))\n","                break\n","\n","            frame_cnt += 1\n","\n","    env.close()\n","    ipythondisplay.clear_output(wait=True)\n","    display.stop()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2UCnQCtph7M","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","    train()"],"execution_count":0,"outputs":[]}]}